---
layout: post
title: XG
---

XG: Executable Graph representations for music
========

Evolutionary generative music based on functional mappings from time
and control variables.

<p> We use a representation for evolutionary music based on free-form
   directed acyclic graphs whose nodes execute arithmetic
   functions. Input nodes supply time variables, abstract control
   variables, and user control signals; multiple output nodes map
   numerical results to MIDI data. The motivation is that multiple
   outputs from a single graph should tend to behave in related ways,
   a key characteristic of good music.

  <p>We get a type of separation of control: the graph specifies the
  musical content, while the control signals specify the musical
  structure. Of course there is also overlap. This separation of music
  into form and content enables novel compositional techniques
  well-suited to writing for games and film, as well as for standalone
  pieces.

<p> Where necessary, evolutionary search can be applied, using
  statistical, target-matching, or purely subjective fitness
  measures.

<p>My thinking on abstract structure in music, and using graphs to
  express functional relationships, was influenced by the
  Buzzmachines.com community, the Buzz machines I wrote for generative
  music, and their use by artists such as
  Tinga: <a href="https://web.archive.org/web/20160817074912/http://buzzmachines.com/viewreview.php?id=1053">This
  example</a> is from 2004.

  <p>This project was also partly inspired by the work of Amy Hoover
  and colleagues on <a href="https://web.archive.org/web/20160817074912/http://eplex.cs.ucf.edu/neatmusic/">NEAT
  Drummer</a>.

<p>

  <hr>


  <h3>Sound and Music Computing 2015</h3>

  <p> I collaborated with Dr Katie Crowley (Trinity College Dublin),
  on an implementation where input signals are taken from
  brain-computer interfaces, ie brain waves control the music. This
  work has been submitted to the Sound and Music Computing Conference
  2015, with the title "Mapping brain signals to music via executable
  graphs". Some example pieces, data files and source code
  are <a href="http://jmmcd.github.io/code/BCI_sample_pieces/">available</a>.

<h3>GECCO 2011</h3> <p> I also collaborated with Prof Una-May O'Reilly
(MIT) on a different implementation, published as "An Executable Graph
Representation for Evolutionary Generative Music", Digital
Entertainment Technologies and Arts Track, GECCO, 2011.


  <p>


XG Demo pieces <a href="code/GraphMusicDemoPieces.tgz">available here</a>.

<p>

XG Experiment 0 (music and
  questionnaire) <a href="code/GraphMusicExperiment0.tgz">available here</a>.

<p>

XG Experiment 1 (music and
  questionnaire) <a href="code/GraphMusicExperiment1.tgz">available here</a>.

<p>

  [Source code available](https://github.com/jmmcd/xg).

<h3>EvoMUSART 2010</h3> <p> On this project I collaborated with Dr
Jianhua Shao (at the time an undergrad summer intern), who implemented
the Jive system
(<a href="https://web.archive.org/web/20160817074912/http://sites.google.com/site/odcsssjian2009/">project
page</a>), where input signals are taken from mouse or Wiimote. This
work was published as "JIVE: A Generative, Interactive, Virtual,
Evolutionary Music System", EvoMUSART 2010 (best paper award).
</div>
