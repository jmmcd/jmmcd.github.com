Andrew Ng:

> whether an AI system is sentient (able to feel) is a philosophical question rather than a scientific one. A scientific hypothesis must be falsifiable.

I wouldn't agree with this! I agree falsiability is important. This question is in a grey area, because it is certainly impractical to falsify, and maybe even impossible in principle. But there *is* a truth of the matter. Concerning myself, I know that I am conscious. If an AI becomes conscious, it will know it, and will be in possession of a true fact. That sets this question apart from questions such as "what is the meaning of life?", which Andrew Ng mentions in the same essay.






English as the language of thought. What if we fine-tune a language model on verbal arguments which are fairly sound, and start from premises and a given question, and proceed to an answer to that question? The arguments should also bring in facts from the real world, but should introduce them carefully, eg "we assume that X" is a given assumption, and "it is a fact that X" is a new assumption, not given. It should be easy for the human user to verify these facts. A further step would be train it to state high-level arguments first, then break them down as needed. More generally, if we want specific behaviours from a LM (eg seriousness, logic, formal tone, sticking to the topic) then we may need to (a) fine-tune and/or (b) prompt it to do so. If the LM is not properly fine-tuned and/or prompted and we observe that it fails to stick to the topic, or tells a lie, this is not evidence.



Current LM/image models have a type of grounding. What can we do with them? Can we interrogate them and ask them to keep looking at the image? Describe in more detail? Infer the character's motivations?
