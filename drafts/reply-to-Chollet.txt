@fchollet?
@lexfridman

Fran√ßois Chollet had a great conversation with Lex Fridman in Fridman's [AI Podcast, 14 September 2019](.


They both do a great job of discussing things in a way that encourages debate. One thing Chollet does badly is characterise people who disagree with him re AI dangers as being driven by ideology or religion -- I can't remember the phrasing -- Fridman sums it up as not being scientific. That is bad manners in a discussion.

Anyway, there are some points worth engaging with. Chollet says that recursive self-improvement should not be a worry, because as soon as one bottleneck disappears (eg algorithm), another one will kick in (eg data). That's not right, because human-level intelligence is already achievable with current data sources -- by us. Many naive anti-AGI arguments fail immediately because they claim, accidentally, as here, that human-level intelligence is impossible for humans.

He also says that intelligence is inherently embodied -- you can't have intelligence without context, a body, a task. I disagree with this because if I close my eyes I can still think about an abstract problem in graph theory. 

When Fridman asks about AI achieving human-level or general intelligence, Chollet asks -- like Jordan Peterson -- "it depends on what you mean by human-level". Even though the point is fairly clear. The distinction between human-level and human-like is necessary of course. But with that point made, we should now be ready to discuss human-level intelligence without discussing emotions or bodies.

Having said all this, I'm not convinced that there is any type of intelligence higher than what we already have -- just faster versions of the same thing, with more memory.
